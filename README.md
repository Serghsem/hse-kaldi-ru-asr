# Russian Speech Recognition with Kaldi using voxforge_ru dataset
## Instructions

1. Optional: for much better performance of the DNN training process, install the CUDA toolkit (https://developer.nvidia.com/cuda-toolkit) if proper hardware (CUDA GPU) is available.
2. Install the Kaldi toolkit (follow instructions at http://kaldi-asr.org/doc/install.html) and the voxforge_ru (git clone https://github.com/freerussianasr/recipes) recipe for Kaldi. 
3. Add local/nnet2/run\_dnn.sh file (nnet2) to the voxforge\_ru folder and replace path.sh with the one included in this repository. Note that at the time of writing this, path.sh included in voxforge\_ru uses outdated paths for some of Kaldi executables (e. g. arpa2fst - kaldi/src/lm -> kaldi/src/lmbin, cuda-compiled - kaldi/src/nnet2bin), so modifying path.sh is required even while not running nnet2. Set paths for dataset storiing (DATA\_ROOT) and kaldi distribution (KALDI\_ROOT).
4. Run the getdata.sh script from voxforge_ru to download and extract audio dataset.
5. Voxforge\_ru scripts rely on scripts from the voxforge example included in the Kaldi distribution, create links steps to kaldi/egs/voxforge/s5/steps and utils to kaldi/egs/voxforge/s5/utils in the voxforge_ru folder for the scripts to function properly.
6. run.sh script requires the following to be installed: flac (sudo apt-get install flac), SRILM (run kaldi/tools/install\_srilm.sh), SEQUITUR (run kaldi/tools/extras/install\_sequitur.sh).
7. Configure (e. g. set number of parallel jobs) and run the run.sh script.
8. To run the first neural network (included in the voxforge_ru recipe, uses nnet1 implementation of kaldi - can be trained with a single GPU) run local/nnet/run\_dnn.sh from the voxforge\_ru directory. To run the second neural network (modified from the Wall Street Journal example in kaldi distribution, uses nnet2 implementation of kaldi - can be trained with multiple GPUs, applies VTLN warping and time-warping to generate more training data) run local/nnet2/run\_dnn.sh from the voxforge\_ru directory. Note that in both cases GMM is assumed to be already generated by the run.sh script.

## Computational results

The WER (Word Error Rate = (I + D + S) / N, where I - number of insertions, D - number of deletions, S - number of substitutions required to get from the correct transcript to the predicted one, N - number of words in the correct transcript) statistic is used to evaluate training results of both neural networks.

#### nnet1 (on different training stages)

dnn5b\_pretrain-dbn_dnn
%WER 11.96 \[ 413 / 3454, 111 ins, 98 del, 204 sub \]
dnn5b\_pretrain-dbn\_dnn\_smbr
%WER 11.46 \[ 396 / 3454, 118 ins, 77 del, 201 sub \]
dnn5b\_pretrain-dbn\_dnn\_smbr\_i1lats
%WER 11.23 \[ 388 / 3454, 113 ins, 69 del, 206 sub \]

#### nnet2

Still computing (to be updated)
